# -*- coding: utf-8 -*-
"""functions.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ld9GIMnXZy2LSy4yFBEfsyb4ZHRiHIf0
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random


# Reproducibility 고려
'''set_seed는 AI의 도움을 받았습니다'''
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class MLP(nn.Module):
    def __init__(self, input_size=784, hidden_size=100, num_classes=10, hidden_layers=2, dropout_rate=0.0):
        super(MLP, self).__init__()

        layers = []

        '''입력층~출력층 부분은 AI로 수정했습니다'''
        # 입력층 -> hidden layer
        layers.append(nn.Linear(input_size, hidden_size))
        layers.append(nn.ReLU())
        if dropout_rate > 0:
            layers.append(nn.Dropout(dropout_rate))

        # 내부 hidden layers
        for _ in range(hidden_layers - 1):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
            if dropout_rate > 0:
                layers.append(nn.Dropout(dropout_rate))

        # hidden layer -> 출력층
        layers.append(nn.Linear(hidden_size, num_classes))

        self.layers = nn.Sequential(*layers)


    def forward(self, x):
        """
        순전파 함수
        x: 입력 텐서 (batch_size, 784)
        return: 출력 텐서 (batch_size, 10)
        """
        return self.layers(x)


# 훈련 함수
def train_model(model, train_loader, test_loader, device, learning_rate, nb_epochs):

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    train_losses = []
    train_accuracies = []
    test_accuracies = []

    for epoch in range(nb_epochs):
        model.train()

        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for batch in train_loader:
            # 데이터를 디바이스로 이동
            imgs = batch["image"].to(device)
            labels = batch["label"].to(device)

            # 그래디언트 초기화
            optimizer.zero_grad()

            # Forward pass
            outputs = model(imgs)
            loss = criterion(outputs, labels)

            # Backward pass
            loss.backward()

            # 파라미터 업데이트
            optimizer.step()

            # 통계 업데이트
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        # 에포크 종료 후 훈련 통계
        epoch_loss = running_loss / len(train_loader)
        epoch_train_acc = 100 * correct_train / total_train
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_train_acc)

        # 테스트 정확도 계산
        model.eval()  # 평가 모드로 설정
        correct_test = 0
        total_test = 0

        with torch.no_grad():  # 그래디언트 계산 비활성화
            for batch in test_loader:
                imgs = batch["image"].to(device)
                labels = batch["label"].to(device)

                outputs = model(imgs)
                _, predicted = torch.max(outputs, 1)
                total_test += labels.size(0)
                correct_test += (predicted == labels).sum().item()

        test_acc = 100 * correct_test / total_test
        test_accuracies.append(test_acc)

        print(f"Epoch {epoch+1}/{nb_epochs}: Train={epoch_train_acc:.2f}%, Test={test_acc:.2f}%")

    return train_losses, train_accuracies, test_accuracies


# 실험 1: 최적 Hidden Size 찾기

def experiment1_hidden_size(hidden_sizes, train_loader, test_loader, device,
                           learning_rate, nb_epochs, seed):
    results = {}
    best_accuracy = 0
    best_hidden_size = None

    for hidden_size in hidden_sizes:
        print(f"\nHidden Size = {hidden_size}")

        set_seed(seed)

        model = MLP(
            input_size=784,
            hidden_size=hidden_size,
            num_classes=10,
            hidden_layers=2,
            dropout_rate=0.0
        ).to(device)

        train_losses, train_accs, test_accs = train_model(
            model, train_loader, test_loader, device, learning_rate, nb_epochs
        )

        final_test_acc = test_accs[-1]
        results[hidden_size] = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'test_accs': test_accs,
            'final_test_acc': final_test_acc
        }
        print(f"최종 Test 정확도: {final_test_acc:.2f}%")

        if final_test_acc > best_accuracy:
            best_accuracy = final_test_acc
            best_hidden_size = hidden_size

    return {
        'results': results,
        'best_hidden_size': best_hidden_size,
        'best_accuracy': best_accuracy,
        'hidden_sizes': hidden_sizes
    }


# 실험 2: 최적 Hidden Layers 개수 구하기
def experiment2_hidden_layers(hidden_layers_list, best_hidden_size, train_loader,
                              test_loader, device, learning_rate, nb_epochs, seed):

    print(f"Hidden Size = {best_hidden_size} (실험1에서 구한 최적값)")
    results = {}
    best_accuracy = 0
    best_hidden_layers = None

    for hidden_layers_count in hidden_layers_list:
        print(f"\nHidden Layers = {hidden_layers_count}")

        set_seed(seed)

        model = MLP(
            input_size=784,
            hidden_size=best_hidden_size,
            num_classes=10,
            hidden_layers=hidden_layers_count,
            dropout_rate=0.0
        ).to(device)

        train_losses, train_accs, test_accs = train_model(
            model, train_loader, test_loader, device, learning_rate, nb_epochs
        )

        final_test_acc = test_accs[-1]
        results[hidden_layers_count] = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'test_accs': test_accs,
            'final_test_acc': final_test_acc
        }
        print(f"최종 Test 정확도: {final_test_acc:.2f}%")

        if final_test_acc > best_accuracy:
            best_accuracy = final_test_acc
            best_hidden_layers = hidden_layers_count

    return {
        'results': results,
        'best_hidden_layers': best_hidden_layers,
        'best_accuracy': best_accuracy,
        'hidden_layers_list': hidden_layers_list,
        'hidden_size': best_hidden_size
    }


# 실험 3: 최적 Dropout 비율 구하기

def experiment3_dropout(dropout_rates, best_hidden_size, best_hidden_layers,
                       train_loader, test_loader, device, learning_rate, nb_epochs, seed):
    print(f"Hidden Size = {best_hidden_size} (실험1에서 구한 최적값)")
    print(f"Hidden Layers = {best_hidden_layers} (실험2에서 구한 최적값)")
    results = {}
    best_accuracy = 0
    best_dropout = None

    for dropout in dropout_rates:
        print(f"\nDropout = {dropout}")

        set_seed(seed)

        model = MLP(
            input_size=784,
            hidden_size=best_hidden_size,
            num_classes=10,
            hidden_layers=best_hidden_layers,
            dropout_rate=dropout
        ).to(device)

        train_losses, train_accs, test_accs = train_model(
            model, train_loader, test_loader, device, learning_rate, nb_epochs
        )

        final_test_acc = test_accs[-1]
        results[dropout] = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'test_accs': test_accs,
            'final_test_acc': final_test_acc
        }
        print(f"최종 Test 정확도: {final_test_acc:.2f}%")

        if final_test_acc > best_accuracy:
            best_accuracy = final_test_acc
            best_dropout = dropout

    return {
        'results': results,
        'best_dropout': best_dropout,
        'best_accuracy': best_accuracy,
        'dropout_rates': dropout_rates,
        'hidden_size': best_hidden_size,
        'hidden_layers': best_hidden_layers
    }


# 실험 4: Dropout + Hidden Layers 개수 조합 실험
def experiment4_dropout_with_layers(hidden_layers_list, best_hidden_size, best_dropout,
                                   train_loader, test_loader, device,
                                   learning_rate, nb_epochs, seed):
    print(f"Hidden Size = {best_hidden_size} (실험1에서 구한 최적값)")
    print(f"Dropout = {best_dropout} (실험3에서 구한 최적값)")
    results = {}
    best_accuracy = 0
    best_hidden_layers = None

    for hidden_layers_count in hidden_layers_list:
        print(f"\nHidden Layers = {hidden_layers_count}, Dropout = {best_dropout}")

        set_seed(seed)

        model = MLP(
            input_size=784,
            hidden_size=best_hidden_size,
            num_classes=10,
            hidden_layers=hidden_layers_count,
            dropout_rate=best_dropout
        ).to(device)

        train_losses, train_accs, test_accs = train_model(
            model, train_loader, test_loader, device, learning_rate, nb_epochs
        )

        final_test_acc = test_accs[-1]
        results[hidden_layers_count] = {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'test_accs': test_accs,
            'final_test_acc': final_test_acc
        }
        print(f"최종 Test 정확도: {final_test_acc:.2f}%")

        if final_test_acc > best_accuracy:
            best_accuracy = final_test_acc
            best_hidden_layers = hidden_layers_count

    return {
        'results': results,
        'best_hidden_layers': best_hidden_layers,
        'best_accuracy': best_accuracy,
        'hidden_layers_list': hidden_layers_list,
        'hidden_size': best_hidden_size,
        'dropout': best_dropout
    }