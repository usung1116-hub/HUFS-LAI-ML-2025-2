# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 97.76%
- 훈련 시간: 46초

## 실험 결과
### 실험 1: [하이퍼파라미터 튜닝]
- 변경사항: 학습률은 0.01, 0.001, 0.0001으로 총 세 가지 값을 사용하였고, 은닉층의 크기를 [128, 64], [200, 50], [256, 128]로 총 세 가지 값을 사용하여 총 9번의 실험을 진행하였습니다. 
- 결과: 학습률: 0.001, 은닉층: [200, 50]의 조합이 97.70%의 정확도로 가장 좋은 결과를 보였습니다.
- 분석: 학습률 0.001이 다른 값들에 비해 안정적이고 높은 성능을 보였습니다. 학습률 0.01은 너무 높아 최적점을 놓쳤을 가능성이 있고, 0.0001은 5 에포크 내에 충분히 학습하지 못했을 수 있습니다.
흥미롭게도, [256, 128] 구조보다 첫 번째 층이 넓고 두 번째 층이 좁은 [200, 50] 구조가 더 좋은 성능을 보였습니다. 이를 통해 모델이 첫 번째 레이어에서 풍부한 특징을 추출한 후, 두 번째 레이어에서 이를 효과적으로 압축하며 과적합을 일부 방지했을 가능성을 상정할 수 있습니다.

### 실험 2: [모델 구조 개선]
- 변경사항: [128, 64], [256, 128, 64], [512, 256, 128, 64]로 은닉층의 깊이를 달리하였고, dropout에서 0.0, 0.2, 0.4로 총 3개의 값을 사용하여 총 9번의 실험을 진행하였습니다.
- 결과: [512, 256, 128, 64]의 은닉층과 0.2의 dropout 값의 조합이 97.98%의 정확도로 가장 좋은 결과를 보였습니다.
- 분석: 모델의 깊이가 깊어질수록(파라미터 수가 많아질수록) 성능이 향상되는 경향을 보였고, 마찬가지로 가장 깊은 4층 구조가 가장 높은 성능을 달성했습니다. 가장 복잡한 모델이 드롭아웃 비율 0.2와 결합되었을 때 최고의 성능을 냈다는 점이 인상깊었습니다. 이는 모델이 복잡해지면서 발생할 수 있는 과적합을 드롭아웃이 효과적으로 억제하여, 모델의 일반화 성능을 크게 향상시켰음을 의미합니다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 단순히 학습률이나 구조를 변경하는 하이퍼파라미터 튜닝보다는, 은닉층 추가나 Dropout 추가와 같은 모델 구조 개선이 성능 향상에 더 효과적이라는 결론을 도출할 수 있습니다. 
- 관찰된 패턴: 복잡한 모델일수록 과적합의 위험이 크지만, 드롭아웃을 통해 이를 제어하면 모델의 잠재력을 최대한 활용할 수 있습니다.
0.001의 학습률은 MNIST 데이터셋에서 가장 안정적인 선택지임을 알 수 있었습니다.
- 추가 개선 아이디어: ReLU 대신 LeakyReLU나 GeLU 같은 다른 활성화 함수를 테스트해보는 것도 좋을 것 같습니다. 또한, 현재 5 에포크로 설정되어 있으므로, 학습을 더 진행하여 수렴 여부를 확인하면 좋을 것 같습니다.